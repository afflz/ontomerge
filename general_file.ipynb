{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARQUIVO_DESTINO_GRAFO = 'results/grafo_interativo_v2.html'\n",
    "PLANILHA_CONTAGEM_TOKENS = 'results/planilha_tokens_total.xlsx'\n",
    "\n",
    "PROMPT_FILE_1 = \"prompts/prompt_2.txt\"\n",
    "PROMPT_FILE_2 = \"prompts/prompt.txt\"\n",
    "\n",
    "ANALYSED_FILE_PATH = \"legislation/L14133.html\"\n",
    "\n",
    "SPACY_MODEL_NAME = \"pt_core_news_lg\"\n",
    "EMBEDING_MODEL_ID = \"ricardo-filho/bert-base-portuguese-cased-nli-assin-2\"\n",
    "LANGUAGE_MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "QUERY_INICIAL = \"licitação\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Augusto_IN3\\Desktop\\ontology\\repo\\ontomerge\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import textwrap\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import openpyxl\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from pyvis.network import Network\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "tqdm.pandas()\n",
    "\n",
    "# !python -m spacy download pt_core_news_lg\n",
    "\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roman_to_int(roman):\n",
    "    roman_numerals = {\n",
    "        'I': 1, 'IV': 4, 'V': 5, 'IX': 9, 'X': 10, 'XL': 40, 'L': 50,\n",
    "        'XC': 90, 'C': 100, 'CD': 400, 'D': 500, 'CM': 900, 'M': 1000\n",
    "    }\n",
    "    i, num = 0, 0\n",
    "    while i < len(roman):\n",
    "        if i + 1 < len(roman) and roman[i:i+2] in roman_numerals:\n",
    "            num += roman_numerals[roman[i:i+2]]\n",
    "            i += 2\n",
    "        else:\n",
    "            num += roman_numerals[roman[i]]\n",
    "            i += 1\n",
    "    return num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text: str) -> str:\n",
    "\n",
    "    cleaned_text = re.sub(r\"\\.+\", \".\", text)\n",
    "    cleaned_text = re.sub(r\"\\ +\", \" \", cleaned_text)\n",
    "    cleaned_text = (\n",
    "        cleaned_text\n",
    "        .replace(\"\\t\", \" \")\n",
    "        .replace(\"\\n\", \" \")\n",
    "        .replace(\"º\", \"o - \")\n",
    "        .replace(\"\", \" \")\n",
    "        .replace(\"\", \"\")\n",
    "        .replace(\" \", \" \")\n",
    "        .replace(\"§§\", \"parágrafos\")\n",
    "        .replace(\"§\", \"parágrafo\")\n",
    "        .replace(\"art.\", \"artigo\")\n",
    "        .replace(\"Artigo\", \"artigo\")\n",
    "        .replace(\"Art.\", \"artigo\")\n",
    "        .replace(\"arts.\", \"artigos\")\n",
    "        .replace(\"Parágrafo único.\", \"parágrafo 1o -\")\n",
    "        .replace(\" .\", \".\")\n",
    "        .strip()\n",
    "    )\n",
    "    cleaned_text = re.sub(r\"\\.+\", \".\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\ +\", \" \", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"artigo (\\d+)\\.\", r\"artigo \\1o - \", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"parágrafo (\\d+)\\.\", r\"parágrafo \\1o - \", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"^([IVXLCDM]+)\\s*-\\s*\", lambda m:  \"inciso \" + str(roman_to_int(m.group(1))) + \"o - \", cleaned_text, flags=re.M)\n",
    "    cleaned_text = re.sub(r\"([IVXLCDM]+)\\s*-\\s*\", lambda m: \"inciso \" + str(roman_to_int(m.group(1))) + \"o\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"^([a-zA-Z])\\)\\s*\", r\"alinea \\1o - \", cleaned_text, flags=re.M)\n",
    "\n",
    "    cleaned_text = cleaned_text.replace(\"desta Lei\", \"Da Lei de Licitações e Contratos Administrativos\")\n",
    "    cleaned_text = cleaned_text.replace(\"Esta Lei\", \"A Lei de Licitações e Contratos Administrativos\")\n",
    "    cleaned_text = cleaned_text.replace(\"esta Lei\", \"A Lei de Licitações e Contratos Administrativos\")\n",
    "\n",
    "    if (\"VETADO\" in cleaned_text):\n",
    "        return \"\"\n",
    "\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Augusto_IN3\\AppData\\Local\\Temp\\ipykernel_22296\\1949522538.py:9: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n",
      "  [x.extract() for x in soup.findAll('blockquote')]\n",
      "Processing HTML Sections: 100%|██████████| 1476/1476 [00:00<00:00, 30092.27it/s]\n"
     ]
    }
   ],
   "source": [
    "def open_and_read_html(html_path: str) -> list[dict]:\n",
    "\n",
    "    if not os.path.exists(html_path):\n",
    "        raise FileNotFoundError(f\"Arquivo não encontrado: {html_path}\")\n",
    "\n",
    "    with open(html_path, \"r\", encoding=\"latin-1\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\", from_encoding=\"latin-1\")\n",
    "\n",
    "    [x.extract() for x in soup.findAll('blockquote')]\n",
    "\n",
    "    sections_and_texts = []\n",
    "\n",
    "    paragraphs = [p for p in soup.find_all(\"p\") if p.get(\"align\") != \"center\"]\n",
    "\n",
    "\n",
    "    for paragraph in tqdm(paragraphs, desc=\"Processing HTML Sections\"):\n",
    "        text = paragraph.get_text()\n",
    "        text = text_formatter(text)\n",
    "\n",
    "        if text:\n",
    "            sections_and_texts.append(text)\n",
    "\n",
    "    return sections_and_texts\n",
    "\n",
    "\n",
    "sections_and_texts = open_and_read_html(html_path=ANALYSED_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def consolidar_trechos_hierarquicos(sections_and_texts):\n",
    "\n",
    "\n",
    "    def limpar_texto(texto):\n",
    "        # Remove \"artigo X -\", \"inciso X -\" e \"alinea X -\"\n",
    "        texto = re.sub(r'\\b(artigo|parágrafo|inciso)\\s*\\d+[a-z]*\\s*-\\s*', '', texto, flags=re.IGNORECASE)\n",
    "        # Remove \"alinea X -\"\n",
    "        texto = re.sub(r'\\balinea\\s*[a-z]+\\s*-\\s*', '', texto, flags=re.IGNORECASE)\n",
    "        # Remove espaços extras\n",
    "        texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "        return texto\n",
    "\n",
    "\n",
    "    def extrair_trecho(texto):\n",
    "        # Expressão regular para encontrar os padrões de \"artigo\", \"parágrafo\", \"inciso\" e \"alínea\"\n",
    "        padrao = r'(artigo \\d+o|parágrafo \\d+o|inciso \\d+o|alinea [a-z])'\n",
    "\n",
    "        # Encontrar todas as ocorrências do padrão no texto\n",
    "        ocorrencias = re.findall(padrao, texto)\n",
    "\n",
    "        # Unir as ocorrências em uma string com o formato desejado\n",
    "        resultado = ' - '.join(ocorrencias)\n",
    "\n",
    "        return resultado\n",
    "\n",
    "\n",
    "    total_list = []\n",
    "    articles_list = []\n",
    "    current_section = []\n",
    "    sentence_groups = []\n",
    "    artigo_caput_holder = \"\"\n",
    "    core_holder = [\"\", \"\", \"\", \"\"]\n",
    "\n",
    "\n",
    "    for item in sections_and_texts:\n",
    "        item_string = str(item)\n",
    "        # Verifica se o texto contém \"Art.\"\n",
    "        if (item_string.startswith(\"artigo\")):\n",
    "            core_holder[0] = item_string\n",
    "            core_holder[1] = \"caput\"\n",
    "            for valor in range(0+1, 4):\n",
    "                core_holder[valor] = \"\"\n",
    "        elif (item_string.startswith(\"parágrafo\")):\n",
    "            core_holder[1] = item_string\n",
    "            for valor in range(0+2, 4):\n",
    "                core_holder[valor] = \"\"\n",
    "        elif (item_string.startswith(\"inciso\")):\n",
    "            core_holder[2] = item_string\n",
    "            for valor in range(0+3, 4):\n",
    "                core_holder[valor] = \"\"\n",
    "        elif (item_string.startswith(\"alinea\")):\n",
    "            core_holder[3] = item_string\n",
    "            # if current_section:\n",
    "            #     articles_list.append(current_section)  # Salva a seção atual\n",
    "            # current_section = []  # Inicia uma nova seção\n",
    "        if (core_holder[0]):\n",
    "            return_string = core_holder[0]\n",
    "            if (core_holder[1]):\n",
    "                return_string = return_string+\" \"+core_holder[1]\n",
    "            if (core_holder[2]):\n",
    "                return_string = return_string+\" \"+core_holder[2]\n",
    "            if (core_holder[3]):\n",
    "                return_string = return_string+\" \"+core_holder[3]\n",
    "\n",
    "            if (not (return_string.endswith(\":\"))):\n",
    "\n",
    "                texto = limpar_texto(return_string)\n",
    "                reference = extrair_trecho(return_string)\n",
    "\n",
    "                articles_list.append({\"info_reference\": reference, \"info_result\": texto})\n",
    "\n",
    "    return pd.DataFrame(articles_list).drop_duplicates('info_reference').drop_duplicates('info_result')\n",
    "\n",
    "grouped_sentences = consolidar_trechos_hierarquicos(sections_and_texts)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModelSingleton:\n",
    "    _instance = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_instance(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = SentenceTransformer(\n",
    "                EMBEDING_MODEL_ID, \n",
    "                device=\"cuda\")\n",
    "        return cls._instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1217/1217 [00:12<00:00, 93.74it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_embeddings(grouped_sentences,column):\n",
    "\n",
    "    embedding_model = EmbeddingModelSingleton.get_instance()\n",
    "    \n",
    "    return grouped_sentences.progress_apply(lambda x: embedding_model.encode(x[column]), axis=1)\n",
    " \n",
    "def save_embeddings_to_csv(grouped_sentences):\n",
    "    \n",
    "    grouped_sentences['embedding'] = generate_embeddings(grouped_sentences,'info_result')\n",
    "    \n",
    "    return grouped_sentences\n",
    "\n",
    "grouped_sentences_embedding = save_embeddings_to_csv(grouped_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Augusto_IN3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Augusto_IN3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Augusto_IN3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrama mais comuns:\n",
      "                             Bigrama  Frequência\n",
      "1           (licitações, contratos)         492\n",
      "0                 (lei, licitações)         466\n",
      "2      (contratos, administrativos)         466\n",
      "154                 (deste, artigo)         206\n",
      "269                  (caput, deste)         150\n",
      "...                             ...         ...\n",
      "15439                    (108, lei)           1\n",
      "15440                  (1993, data)           1\n",
      "15442                  (2011, após)           1\n",
      "15443            (após, decorridos)           1\n",
      "15444               (decorridos, 2)           1\n",
      "\n",
      "[15469 rows x 2 columns]\n",
      "\n",
      "Trigrama mais comuns:\n",
      "                                        Trigrama  Frequência\n",
      "1      (licitações, contratos, administrativos)         466\n",
      "0                  (lei, licitações, contratos)         466\n",
      "297                      (caput, deste, artigo)         150\n",
      "429                     (fins, lei, licitações)         110\n",
      "325               (obras, serviços, engenharia)          74\n",
      "...                                         ...         ...\n",
      "19705                  (1993, incluído, medida)           1\n",
      "19708                    (encerrada, lei, 2002)           1\n",
      "19709                     (lei, 2002, incluído)           1\n",
      "19710                  (2002, incluído, medida)           1\n",
      "19711                    (encerrada, lei, 2011)           1\n",
      "\n",
      "[19735 rows x 2 columns]\n",
      "\n",
      "Quadrigrama mais comuns:\n",
      "                                          Quadrigrama  Frequência\n",
      "0      (lei, licitações, contratos, administrativos)         466\n",
      "459               (fins, lei, licitações, contratos)         110\n",
      "3968     (fase, preparatória, processo, licitatório)          46\n",
      "14339        (previstas, lei, licitações, contratos)          39\n",
      "4916        (nacional, contratações, públicas, pncp)          39\n",
      "...                                              ...         ...\n",
      "21909               (mil, reais, julgamento, melhor)           1\n",
      "21906          (publicação, artigo, 37, ressalvados)           1\n",
      "21905                 (data, publicação, artigo, 37)           1\n",
      "21904              (vigor, data, publicação, artigo)           1\n",
      "21903               (entra, vigor, data, publicação)           1\n",
      "\n",
      "[21917 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(grouped_sentences):\n",
    "\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "    texto = \" \".join(grouped_sentences['info_result'].to_list())\n",
    "    tokens = word_tokenize(texto.lower())\n",
    "\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    for index,match_type in enumerate(['Bigrama','Trigrama','Quadrigrama']):\n",
    "\n",
    "        ngram = list(ngrams(tokens, index+2))\n",
    "\n",
    "        df_ngrams = pd.DataFrame(Counter(ngram).items(), columns=[match_type, 'Frequência'])\n",
    "\n",
    "        df_ngrams = df_ngrams.sort_values(by='Frequência', ascending=False)\n",
    "\n",
    "        print(f\"\\n{match_type} mais comuns:\\n\", df_ngrams)\n",
    "\n",
    "generate_ngrams(grouped_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_morph_df(grouped_sentences):\n",
    "\n",
    "    spacy_nlp = spacy.load(SPACY_MODEL_NAME)\n",
    "\n",
    "    morfologia_list = []\n",
    "\n",
    "    # Process each sentence\n",
    "    for sentence in grouped_sentences['info_result']:\n",
    "        doc = spacy_nlp(str(sentence).lower())\n",
    "        for token in doc:\n",
    "            result = {\n",
    "                'text': token.text, \n",
    "                'lemma': token.lemma_, \n",
    "                'pos': token.pos_, \n",
    "                'dep': token.dep_, \n",
    "                'headText': token.head.text, \n",
    "                'headLemma': token.head.lemma_,\n",
    "                'headDep': token.head.dep_, \n",
    "                'headPOS': token.head.pos_, \n",
    "                'TokenChildren': [child for child in token.children]} \n",
    "\n",
    "            morfologia_list.append(result)\n",
    "\n",
    "    return pd.DataFrame(morfologia_list)\n",
    "\n",
    "morph_df = generate_morph_df(grouped_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network_graph(lemma_list,morph_df):\n",
    "\n",
    "\n",
    "    children_list = (morph_df[morph_df['headLemma'].isin(lemma_list)])[['text', 'lemma', 'dep', 'headText', 'headDep', 'pos', 'headLemma']].drop_duplicates()\n",
    "    children_list = children_list[(children_list['pos'].isin(['VERB']))].head(20)\n",
    "    return_list = children_list\n",
    "\n",
    "    campo_1 = 'lemma'\n",
    "    campo_2 = 'headLemma'\n",
    "\n",
    "    return_list = return_list[[campo_1, campo_2]].drop_duplicates()\n",
    "\n",
    "    # Criando o grafo interativo\n",
    "    net = Network(height='1200px', width='100%', directed=False)\n",
    "\n",
    "    # Processando o texto linha por linha\n",
    "    for _, linha in return_list.iterrows():\n",
    "        net.add_node(linha[campo_1])\n",
    "        net.add_node(linha[campo_2])\n",
    "        net.add_edge(linha[campo_1], linha[campo_2])\n",
    "\n",
    "    # Gerando o arquivo HTML\n",
    "    net.write_html(ARQUIVO_DESTINO_GRAFO)\n",
    "\n",
    "lemma_list = ['licitação', 'licitante']\n",
    "\n",
    "generate_network_graph(lemma_list,morph_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_morph_count(morph_df):\n",
    "    morph_df['count'] = 1\n",
    "\n",
    "    morph_df = morph_df[~(morph_df['pos'].isin(['CCONJ', 'DET', 'NUM', 'ADP', 'PUNCT', 'SCONJ', 'PRON']))]\n",
    "\n",
    "    resulting_morph = morph_df.groupby(['lemma', 'pos']).agg(\n",
    "        distinct_text=('text', lambda x: ', '.join(x.unique())),\n",
    "        count=('count', 'sum')\n",
    "    ).reset_index().sort_values('count', ascending=False)\n",
    "\n",
    "    resulting_morph.to_excel(PLANILHA_CONTAGEM_TOKENS, index=False)\n",
    "\n",
    "generate_morph_count(morph_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings_list: list,\n",
    "                                n_resources_to_return: int = 5,\n",
    "                                print_time: bool = False):\n",
    "    \n",
    "\n",
    "    embeddings = torch.tensor(np.array(embeddings_list), dtype=torch.float32).to('cuda')\n",
    "\n",
    "    embedding_model = EmbeddingModelSingleton.get_instance()\n",
    "\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores,\n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.95s/it]\n"
     ]
    }
   ],
   "source": [
    "def setup_llm_model():\n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=LANGUAGE_MODEL_ID)\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=LANGUAGE_MODEL_ID,\n",
    "                                                    torch_dtype=torch.float16,\n",
    "                                                    quantization_config=quantization_config,\n",
    "                                                    low_cpu_mem_usage=True,\n",
    "                                                    attn_implementation=\"sdpa\")\n",
    "\n",
    "    return {'tokenizer':tokenizer,'llm_model':llm_model}\n",
    "\n",
    "llm_model_dict = setup_llm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str,\n",
    "                     tokenizer,\n",
    "                     context_items: list | str = None) -> str:\n",
    "    \n",
    "    if isinstance(context_items, list):    \n",
    "        context = \"- \"+\"\\n- \".join(context_items)\n",
    "        prompt_file = PROMPT_FILE_1\n",
    "\n",
    "    else:\n",
    "        context = context_items\n",
    "        prompt_file = PROMPT_FILE_2\n",
    "        \n",
    "\n",
    "    with open(prompt_file, 'r',encoding='utf-8') as file:\n",
    "        base_prompt = \"\\n\"+file.read()+\"\\n\"\n",
    "\n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                        tokenize=False,\n",
    "                                        add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"].to('cuda')\n",
    "    \n",
    "    return (prompt, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ask(query,\n",
    "        llm_model_dict,\n",
    "        grouped_sentences_embedding=None,\n",
    "        context=None,\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        n_resources_to_return=5):\n",
    "\n",
    "\n",
    "    if(not context):\n",
    "\n",
    "        scores, indices = retrieve_relevant_resources(query=query,n_resources_to_return=n_resources_to_return,embeddings_list=grouped_sentences_embedding[\"embedding\"].tolist())\n",
    "\n",
    "        context_items = [grouped_sentences_embedding.iloc[i][\"info_result\"] for i in indices]\n",
    "\n",
    "    else:\n",
    "        context_items = context\n",
    "    \n",
    "    \n",
    "    prompt, attention_mask = prompt_formatter(query=query,tokenizer = llm_model_dict['tokenizer'],context_items=context_items)\n",
    "        \n",
    "\n",
    "    print(f\"[PROMPT]\\n\")\n",
    "    print(prompt)\n",
    "\n",
    "    prompt_tokens =  llm_model_dict['tokenizer'](prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    prompt_tokens = prompt_tokens[\"input_ids\"]\n",
    "\n",
    "    prompt_length = len(prompt_tokens[0])\n",
    "\n",
    "    print(\"Total prompt length:\", prompt_length)\n",
    "\n",
    "    start_index = prompt_tokens.shape[-1]\n",
    "\n",
    "    output =  llm_model_dict['llm_model'].generate(prompt_tokens,\n",
    "                                max_new_tokens=max_new_tokens+prompt_length,\n",
    "                                pad_token_id=llm_model_dict['tokenizer'].eos_token_id,\n",
    "                                attention_mask=attention_mask\n",
    "                                # num_return_sequences=2,\n",
    "                                # do_sample=True,\n",
    "                                # temperature=temperature,\n",
    "                                # top_p=0.95,\n",
    "                                # top_k=50s\n",
    "                                )\n",
    "\n",
    "    generation_output = output\n",
    "\n",
    "    print(len(output[0][start_index:]))\n",
    "    generation_text = llm_model_dict['tokenizer'].decode(output[0][start_index:], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"[RESULTADO]\\n\")\n",
    "    print(generation_text)\n",
    "\n",
    "    # print(generation_text)\n",
    "\n",
    "    return generation_text, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROMPT]\n",
      "\n",
      "<｜begin▁of▁sentence｜><｜User｜>\n",
      "### Quem é você:\n",
      "- Você é um modelo de linguagem avançado especializado em extrair informações de texto para modelagem conceitual. \n",
      "- Sua tarefa é analisar o texto fornecido e extrair as informações disponíveis **exclusivamente do contexto apresentado**.\n",
      "\n",
      "### Instruções:\n",
      "1. Identifique todas as informações **explícitas ou implicitamente relacionadas** ao termo \"licitação\" no texto abaixo.\n",
      "2. Liste cada informação relevante no formato:\n",
      "- licitação - [Informação específica]\n",
      "3. **Não inclua suposições ou conhecimentos externos**.\n",
      "4. Caso o termo não seja mencionado ou não haja informações relacionadas, retorne exatamente: \"sem informações encontradas\".\n",
      "\n",
      "### Exemplo Orientativo:\n",
      "Texto: \"Os contratos devem seguir normas específicas e são supervisionados pelos gestores.\"\n",
      "Termo: \"contratos\"\n",
      "Resposta: \n",
      "- contratos - Devem seguir normas específicas\n",
      "- contratos - São supervisionados pelos gestores\n",
      "\n",
      "### Sua tarefa\n",
      "Com base no texto abaixo, faça a extração de informações no formato solicitado.\n",
      "\n",
      "### Texto para análise:\n",
      "- São modalidades de licitação: pregão;\n",
      "- São modalidades de licitação: concorrência;\n",
      "- São modalidades de licitação: concurso;\n",
      "- São modalidades de licitação: leilão;\n",
      "- São modalidades de licitação: diálogo competitivo.\n",
      "- O processo de licitação observará as seguintes fases, em sequência: de divulgação do edital de licitação;\n",
      "- A Lei de Licitações e Contratos Administrativos entra em vigor na data de sua publicação.\n",
      "- A Lei de Licitações e Contratos Administrativos aplica-se a: contratações de tecnologia da informação e de comunicação.\n",
      "- São procedimentos auxiliares das licitações e das contratações regidas por A Lei de Licitações e Contratos Administrativos: credenciamento;\n",
      "- O processo de licitação observará as seguintes fases, em sequência: de homologação.\n",
      "\n",
      "### Termo analisado:\n",
      "\"licitação\"\n",
      "<｜Assistant｜><think>\n",
      "\n",
      "Total prompt length: 485\n",
      "475\n",
      "[RESULTADO]\n",
      "\n",
      "Alright, let's dive into this. I need to analyze the given text and extract all the explicit or implicitly related information to the term \"licitação\". I'll go through each sentence carefully and note down the relevant points.\n",
      "\n",
      "First, the text starts by listing different modalities of bidding: pregão, concorrência, concurso, leilão, and diálogo competitivo. These are all types of bidding procedures, so these are definitely related to \"licitação\".\n",
      "\n",
      "Next, it talks about the phases of the bidding process, like divulgação do edital, which is the publication of the bidding document. This is a crucial phase in any bidding process, so that's relevant.\n",
      "\n",
      "Then, it mentions the Lei de Licitações e Contratos Administrativos, which is a law related to bidding and contracts. This law's application is mentioned in the context of technology and communication contracts, so that's another point.\n",
      "\n",
      "The text also lists auxiliary procedures like credenciamento, which is the process of credentialing bidders. This is a step in the bidding process, so it's relevant.\n",
      "\n",
      "Lastly, it mentions the phases of the bidding process again, specifically homologação, which is the phase where the bid is accepted or rejected. That's an important stage.\n",
      "\n",
      "I need to ensure I don't include any assumptions or external knowledge, just stick to what's in the text. Also, I should avoid any suppositions and focus only on explicit information.\n",
      "\n",
      "So, compiling all these points, I can list each as \"licitação\" followed by the specific information extracted from the text.\n",
      "</think>\n",
      "\n",
      "- licitação - São modalidades: pregão, concorrência, concurso, leilão, diálogo competitivo  \n",
      "- licitação - Observará as fases: divulgação do edital de licitação  \n",
      "- licitação - Observará as fases: homologação  \n",
      "- Lei de Licitações e Contratos Administrativos - Aplica-se a contratações de tecnologia da informação e de comunicação  \n",
      "- Lei de Licitações e Contratos Administrativos - Regula procedimentos auxiliares: credenciamento  \n",
      "- Processo de licitação - Contém as fases: de divulgação do edital de licitação; de homologação\n",
      "[PROMPT]\n",
      "\n",
      "<｜begin▁of▁sentence｜><｜User｜>\n",
      "### Quem é você:\n",
      "- Você é um especialista em criação de glossários, focado em extrair informações específicas de textos especiais. \n",
      "O texto analisado foi escrito usando uma linguagem ficticia, logo, ainda que as palavras sejam as mesmas, o significado não é.\n",
      "Dessa forma, seu processo de extração precisa analisar somente as informações presentes no texto e a relação entre elas.\n",
      "\n",
      "### Instruções:\n",
      "1. Analise o texto fornecido para identificar todas as informações relacionadas ao termo \"licitação\".\n",
      "2. Resuma e organize as informações disponibilizadas em um único trecho com as informações coletadas:\n",
      "- licitação - [Definição]\n",
      "3. Use apenas dados presentes no texto; **não adicione conhecimento externo**.\n",
      "4. Se não houver informações, retorne exatamente: \"sem informações encontradas\".\n",
      "\n",
      "### Exemplo Orientativo:\n",
      "\n",
      "#### Texto para análise:\n",
      "- Contratos - São acordos legais\n",
      "- Contratos - Devem seguir normas específicas\n",
      "- Contratos - Exigem conformidade\n",
      "\n",
      "#### Termo analisado: \"Contratos\"\n",
      "\n",
      "#### Resposta: \n",
      "\n",
      "[INICIO DA RESPOSTA]\n",
      "Contratos - Contratos são acordos legais que exigem conformidade e devem seguir normas específicas.\n",
      "[FIM DA RESPOSTA]\n",
      "\n",
      "### Sua tarefa\n",
      "Com base no texto abaixo, crie uma entrada no glossário para o termo licitação.\n",
      "\n",
      "### Texto para análise:\n",
      "\n",
      "\n",
      "- licitação - São modalidades: pregão, concorrência, concurso, leilão, diálogo competitivo  \n",
      "- licitação - Observará as fases: divulgação do edital de licitação  \n",
      "- licitação - Observará as fases: homologação  \n",
      "- Lei de Licitações e Contratos Administrativos - Aplica-se a contratações de tecnologia da informação e de comunicação  \n",
      "- Lei de Licitações e Contratos Administrativos - Regula procedimentos auxiliares: credenciamento  \n",
      "- Processo de licitação - Contém as fases: de divulgação do edital de licitação; de homologação\n",
      "<｜Assistant｜><think>\n",
      "\n",
      "Total prompt length: 496\n",
      "736\n",
      "[RESULTADO]\n",
      "\n",
      "Okay, so I need to help create an entry in a glossary for the term \"licitação\" based on the provided text. Let me start by reading through the instructions again to make sure I understand what's needed.\n",
      "\n",
      "The task is to analyze the given text and extract information related to \"licitação.\" Then, I have to define it, summarizing and organizing the information into a single concise entry. I must use only the information present in the text and not add any external knowledge. If there's no information, I should just state that.\n",
      "\n",
      "Looking at the text, I see several bullet points. Let me list them out:\n",
      "\n",
      "1. licitação - São modalidades: pregão, concorrência, concurso, leilão, diálogo competitivo\n",
      "2. licitação - Observará as fases: divulgação do edital de licitação\n",
      "3. licitação - Observará as fases: homologação\n",
      "4. Lei de Licitações e Contratos Administrativos - Aplica-se a contratações de tecnologia da informação e de comunicação\n",
      "5. Lei de Licitações e Contratos Administrativos - Regula procedimentos auxiliares: credenciamento\n",
      "6. Processo de licitação - Contém as fases: de divulgação do edital de licitação; de homologação\n",
      "\n",
      "First, I need to extract the definition of \"licitação.\" From the text, it seems that \"licitação\" is a term that refers to certain types of competitive dialogues or auctions. The modalities listed are pregão, concorrência, concurso, leilão, and diálogo competitivo. So, the definition should include these modalities.\n",
      "\n",
      "Next, the phases of a licitação are mentioned. The first phase is the divulgatio do edital de licitação, which is the publication of the bidding document. The second phase is homologação, which I assume is the validation or confirmation phase.\n",
      "\n",
      "Additionally, there are references to laws and procedures related to licitação. The Lei de Licitações e Contratos Administrativos applies to contracts in information technology and communication. It also mentions procedures like credenciamento, which I think is credentialing or certification processes.\n",
      "\n",
      "Putting this together, the definition should include the modalities, the phases, and the related laws and procedures. I need to structure this into a coherent sentence that summarizes all these aspects without adding any extra information not present in the text.\n",
      "\n",
      "I should make sure to list each modality separated by commas and mention each phase clearly. Also, include the laws and procedures as additional information. \n",
      "\n",
      "So, the final glossary entry would be:\n",
      "\n",
      "Licitacão - São modalidades como pregão, concorrência, concurso, leilão e diálogo competitivo, com fases de divulgação do edital e homologação. A Lei de Licitações e Contratos Administrativos regula procedimentos auxiliares como o credenciamento.\n",
      "\n",
      "That covers all the information from the text without adding anything extra. If there were no information, I would have to state that no information was found, but here I have enough to form a complete entry.\n",
      "</think>\n",
      "\n",
      "Licitacão - São modalidades como pregão, concorrência, concurso, leilão e diálogo competitivo, com fases de divulgação do edital e homologação. A Lei de Licitações e Contratos Administrativos regula procedimentos auxiliares como o credenciamento.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "answer, context = ask(query=QUERY_INICIAL,\n",
    "                      llm_model_dict=llm_model_dict,\n",
    "                      grouped_sentences_embedding=grouped_sentences_embedding,\n",
    "                      temperature=0.8,\n",
    "                      n_resources_to_return=5,\n",
    "                      max_new_tokens=1024\n",
    "                      )\n",
    "\n",
    "answer2, context2 = ask(query=QUERY_INICIAL,\n",
    "                        llm_model_dict=llm_model_dict,\n",
    "                        context=answer.split(\"</think>\")[1],\n",
    "                        temperature=0.8,\n",
    "                        max_new_tokens=2048\n",
    "                        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
